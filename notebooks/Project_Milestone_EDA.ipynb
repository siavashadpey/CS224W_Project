{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPr0Ymu8sDyKtpQAuMP9DHU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siavashadpey/CS224W_Project/blob/main/notebooks/Project_Milestone_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis for PDBBind CleanSplit Datasets"
      ],
      "metadata": {
        "id": "1mYeM4Fiyxnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n"
      ],
      "metadata": {
        "id": "yxQYRDFWygT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If GCS data mount is not populated perform following steps in vm:\n",
        "\"\"\"\n",
        "# Stop/Unmount any previous attempts\n",
        "# Try to unmount the directory in case a previous attempt left it mounted\n",
        "# If this fails, it's fine, it just means it wasn't mounted.\n",
        "fusermount -u ~/gcs_pdbbind_mount\n",
        "\n",
        "# If /gcs_pdbbind_mount not visible in file system create dir for local mount point:\n",
        "mkdir ~/gcs_pdbbind_mount\n",
        "\n",
        "# Replace BUCKET_NAME and DIRECTORY_PATH\n",
        "BUCKET_NAME=\"cs224w-2025-mae-gnn-bucket\"\n",
        "DIRECTORY_PATH=\"data/GEMS_pytorch_datasets\"\n",
        "\n",
        "gcsfuse --only-dir \"$DIRECTORY_PATH\" -o allow_other --implicit-dirs \"$BUCKET_NAME\" ~/gcs_pdbbind_mount\n",
        "\n",
        "#Verify the Mount:\n",
        "ls -l ~/gcs_pdbbind_mount\n",
        "\n",
        "# You should see your .pt files listed here, proving the mount worked.\n"
      ],
      "metadata": {
        "id": "r7LcBcq7yphE",
        "outputId": "59d47649-c690-4fd0-f675-ccab513c2ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-308593605.py, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-308593605.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0c17ba98-6741-4855-b4a7-d9affb362cf5",
        "tags": [],
        "outputId": "0164b4fb-11cd-4b9b-d519-834dde74b7af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric pandas matplotlib\n",
        "!pip install \"numpy<2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HeeZ96kEycts"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Set style for better visualization\n",
        "sns.set_theme(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"torch version:\", torch.__version__)\n",
        "print(\"numpy version:\", np.__version__)\n",
        "print(\"pandas version:\", pd.__version__)"
      ],
      "metadata": {
        "id": "nGmcWTW5y6F_",
        "outputId": "2fec397b-8e39-40b5-900b-79dc3a771bac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.8.0+cu126\n",
            "numpy version: 1.26.4\n",
            "pandas version: 2.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "afd47f13-e837-45e4-95fe-47b750417567",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# CONSTANTS\n",
        "\n",
        "# Need to download Dataset.py from GEMS directory locally onto vm first\n",
        "# git clone http://github.com/camlab-ethz/GEMS.git\n",
        "# Define root path of cloned GEMS repo\n",
        "GEMS_REPO_ROOT = os.path.expanduser('~/GEMS')\n",
        "\n",
        "# Define the directory path where pre-processed .pt dataset files are located\n",
        "DATA_DIR = os.path.join(os.path.expanduser('~'), 'gcs_pdbbind_mount')\n",
        "\n",
        "# List of dataset files to analyze\n",
        "DATASET_FILES = [\n",
        "    # '00AEPL_casf2013.pt',\n",
        "    # '00AEPL_casf2013_indep.pt',\n",
        "    '00AEPL_casf2016.pt',\n",
        "    #'00AEPL_casf2016_indep.pt',\n",
        "    '00AEPL_train_cleansplit.pt',\n",
        "    # '00AEPL_train_pdbbind.pt'\n",
        "]\n",
        "\n",
        "DATASET_FILES_DICT = {\n",
        "    'Train': '00AEPL_train_cleansplit.pt',\n",
        "    'Test': '00AEPL_train_cleansplit.pt'\n",
        "}\n",
        "\n",
        "ALL_ATOMS = ['B', 'C', 'N', 'O', 'P', 'S', 'Se', 'metal', 'halogen']\n",
        "\n",
        "ATOM_HYBRIDIZATION_TYPES = [\"HybridizationType.S\", \"HybridizationType.SP\", \"HybridizationType.SP2\", \"HybridizationType.SP2D\", \"HybridizationType.SP3\", \"HybridizationType.SP3D\", \"HybridizationType.SP3D2\", \"HybridizationType.UNSPECIFIED\"]\n",
        "TOTAL_NUM_H_S = [\"Num_H.0\", \"Num_H.1\", \"Num_H.2\", \"Num_H.3\", \"Num_H.4\"]\n",
        "DEGREES = ['Degree.0', 'Degree.1', 'Degree.2', 'Degree.3', 'Degree.4', 'Degree.5', 'Degree.6', 'Degree.7', 'Degree.8', 'Degree.OTHER']\n",
        "CHIRALITIES = ['Chirality.CHI_UNSPECIFIED', 'Chirality.CHI_TETRAHEDRAL_CW', 'Chirality.CHI_TETRAHEDRAL_CCW', 'Chirality.OTHER']\n",
        "\n",
        "AMINO_ACIDS = [\"ALA\", \"ARG\", \"ASN\", \"ASP\", \"CYS\", \"GLN\", \"GLU\", \"GLY\", \"HIS\", \"ILE\", \"LEU\",\n",
        "            \"LYS\", \"MET\", \"PHE\", \"PRO\", \"SER\", \"THR\", \"TRP\", \"TYR\", \"VAL\"]\n",
        "\n",
        "LIGAND_FEATURE_MAP = ALL_ATOMS + [\"IsInRing\"] + ATOM_HYBRIDIZATION_TYPES + \\\n",
        "                    [\"FORMAL_CHARGE\", \"IS_AROMATIC\", \"MASS/100\", ] + TOTAL_NUM_H_S + \\\n",
        "                    DEGREES + CHIRALITIES\n",
        "FULL_FEATURE_MAP = LIGAND_FEATURE_MAP + AMINO_ACIDS\n",
        "\n",
        "EDGE_FEATURE_MAP = [\"COVALENT_BOND\", \"SELF_LOOP\", \"NON-COVALENT_BOND\", \"EDGE_LENGTH/10\", \"EDGE_LENGTH/10\", \"EDGE_LENGTH/10\", \"EDGE_LENGTH/10\" ] + \\\n",
        "                   [\"BOND_TYPE_0\", \"BOND_TYPE_1.0\", \"BOND_TYPE_1.5\", \"BOND_TYPE_2.0\", \"BOND_TYPE_3.0\"] + \\\n",
        "                   [\"IS_CONJUGATED\", \"IS_IN_RING\", \"BOND_STEREO.NONE\", \"BOND_STEREO.ANY\", \"BOND_STEREO.E\", \"BOND_STEREO.Z\", \"BOND_STEREO.CIS\", \"BOND_STEREO.TRANS\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2qZUCseDsx-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Dynamic Import of Custom Datasets Class\n",
        "# Add the directory to Python's search path.\n",
        "print(f\"Attempting to load custom Dataset class...\")\n",
        "\n",
        "try:\n",
        "  # Add GEMS root to sys.path\n",
        "  if GEMS_REPO_ROOT not in sys.path:\n",
        "    sys.path.append(GEMS_REPO_ROOT)\n",
        "    print(f\"Added {GEMS_REPO_ROOT} to sys.path\")\n",
        "\n",
        "    # Import custom Dataset class from the cloned repo\n",
        "    from Dataset import Dataset as GEMS_Dataset\n",
        "    print(\"Successfully imported custom Dataset class\")\n",
        "\n",
        "    # Import the necessarty PyG components\n",
        "    from torch_geometric.data import Data #, Dataset\n",
        "\n",
        "except ImportError as e:\n",
        "    raise RuntimeError(f\"FATAL ERROR importing GEMS Dataset class: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Extraction and Visualization Functions"
      ],
      "metadata": {
        "id": "z_l3zNn2z5rX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "84b65371-18d3-4746-a11a-79e605ea1849",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def extract_metrics(data_list):\n",
        "    \"\"\"Iterates through list of GEMS datasets and extracts metrics.\"\"\"\n",
        "    metrics = []\n",
        "\n",
        "    num_features = data_list[0].x.size(1)\n",
        "    for i, data in enumerate(data_list):\n",
        "        # Affinity is the y label\n",
        "        affinity = data.y.item()\n",
        "\n",
        "        # ID (Assuming is stored in 'id' attribute)\n",
        "        pdb_id = getattr(data, 'id', f'Complex_{i}')\n",
        "\n",
        "        # Extract Graph Size Metrics\n",
        "        # node features (x): shape is [num_atoms, num_features]\n",
        "        num_nodes = data.x.size(0)\n",
        "\n",
        "        # Edge index: shape is: [2, num_edges]\n",
        "        num_edges = data.edge_index.size(1)\n",
        "\n",
        "        [n_nodes, n_ligand_nodes, n_protein_nodes] = data.n_nodes.numpy()\n",
        "\n",
        "        # Isolate ligand rows\n",
        "        ligand_x = data.x[:n_ligand_nodes]\n",
        "\n",
        "        # Slice the tensor to only get the atom on-hot columns\n",
        "        ligand_heavy_atoms_oh = ligand_x[:, :len(ALL_ATOMS)]\n",
        "        is_heavy_atom = (ligand_heavy_atoms_oh.any(dim=1)).float()\n",
        "        heavy_atom_count = is_heavy_atom.sum().item()\n",
        "\n",
        "        # Isolate protein rows\n",
        "        protein_x = data.x[n_ligand_nodes:]\n",
        "\n",
        "        # Slice the tensor to only get the amino acid one-hot columns (last 20 cols)\n",
        "        protein_aa_oh = protein_x[:, num_features - len(AMINO_ACIDS):]\n",
        "        protein_aa_counts = protein_aa_oh.sum(dim=0)\n",
        "\n",
        "        # convert to frequency normalization\n",
        "        protein_aaf = (protein_aa_counts/ n_protein_nodes).cpu().numpy().tolist()\n",
        "\n",
        "        # Feature counts\n",
        "        #feature_counts_tensor = torch.sum(data.x, dim=0)\n",
        "\n",
        "        # Append the aggregated features to the list\n",
        "        record = {\n",
        "            'PDB_ID': pdb_id,\n",
        "            'Affinity': affinity,\n",
        "            'Num_Nodes': num_nodes,\n",
        "            'Num_Ligand_Atoms': n_ligand_nodes,\n",
        "            'Num_Ligand_Heavy_Atoms': heavy_atom_count,\n",
        "            'Num_Protein_Amino_Acid_Nodes': n_protein_nodes,\n",
        "            'Protein_Amino_Acid_Frequency': protein_aaf,\n",
        "            'Num_Edges': num_edges,\n",
        "            'Density': num_edges / num_nodes,\n",
        "        }\n",
        "\n",
        "        # # Add feature counts dynamically\n",
        "        # for j, feature_name in enumerate(FULL_FEATURE_MAP):\n",
        "        #     record[feature_name] = feature_counts_tensor[j].item()\n",
        "\n",
        "        metrics.append(record)\n",
        "\n",
        "    return pd.DataFrame(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DWCE7hs4dGdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "aZgvlcMTdG1S"
      },
      "outputs": [],
      "source": [
        "def extract_node_features(data_list):\n",
        "    \"\"\"Iterates through list of GEMS datasets and extracts node features.\"\"\"\n",
        "    metrics = []\n",
        "\n",
        "    num_features = data_list[0].x.size(1)\n",
        "    for i, data in enumerate(data_list):\n",
        "        [n_nodes, n_ligand_nodes, n_protein_nodes] = data.n_nodes.numpy()\n",
        "\n",
        "        # Isolate ligand rows\n",
        "        ligand_x = data.x[:n_ligand_nodes]\n",
        "\n",
        "        # Slice the tensor to only get the atom on-hot columns\n",
        "        ligand_heavy_atoms_oh = ligand_x[:, :len(ALL_ATOMS)]\n",
        "        is_heavy_atom = (ligand_heavy_atoms_oh.any(dim=1)).float()\n",
        "        heavy_atom_count = is_heavy_atom.sum().item()\n",
        "\n",
        "        # Isolate protein rows\n",
        "        protein_x = data.x[n_ligand_nodes:]\n",
        "\n",
        "        # Slice the tensor to only get the amino acid one-hot columns (last 20 cols)\n",
        "        protein_aa_oh = protein_x[:, num_features - len(AMINO_ACIDS):]\n",
        "        protein_aa_counts = torch.sum(protein_aa_oh, dim=0)\n",
        "\n",
        "        # convert to frequency normalization\n",
        "        protein_aaf = (protein_aa_counts/ n_protein_nodes).cpu().numpy().tolist()\n",
        "\n",
        "        # Feature counts\n",
        "        ligand_feature_counts_tensor = torch.sum(ligand_x, dim=0)\n",
        "        protein_feature_counts_tensor = torch.sum(protein_x, dim=0)\n",
        "\n",
        "        # Append the aggregated features to the list\n",
        "        record = {\n",
        "            'Num_Ligand_Atoms': n_ligand_nodes,\n",
        "            'Num_Ligand_Heavy_Atoms': heavy_atom_count,\n",
        "            'Num_Protein_Amino_Acid_Nodes': n_protein_nodes,\n",
        "            'Protein_Amino_Acid_Frequency': protein_aaf,\n",
        "        }\n",
        "\n",
        "        # Add feature counts dynamically\n",
        "        for j, feature_name in enumerate(LIGAND_FEATURE_MAP):\n",
        "            record[feature_name] = ligand_feature_counts_tensor[j].item()\n",
        "\n",
        "        for j, feature_name in enumerate(AMINO_ACIDS):\n",
        "            record[feature_name] = protein_feature_counts_tensor[j].item()\n",
        "\n",
        "        metrics.append(record)\n",
        "\n",
        "    return pd.DataFrame(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_edge_features(data_list):\n",
        "    \"\"\"Iterates through list of GEMS datasets and extracts edge features.\"\"\"\n",
        "    metrics = []\n",
        "\n",
        "    num_features = data_list[0].x.size(1)\n",
        "    for i, data in enumerate(data_list):\n",
        "        [n_nodes, n_ligand_nodes, n_protein_nodes] = data.n_nodes.numpy()\n",
        "\n",
        "        # Isolate ligand rows\n",
        "        ligand_x = data.x[:n_ligand_nodes]\n",
        "\n",
        "        # Isolate protein rows\n",
        "        protein_x = data.x[n_ligand_nodes:]\n",
        "\n",
        "        # Feature counts\n",
        "        edge_feature_counts_tensor = torch.sum(data.edge_attr, dim=0)\n",
        "\n",
        "        # Append the aggregated features to the list\n",
        "        record = {}\n",
        "\n",
        "        # Add feature counts dynamically\n",
        "        for j, feature_name in enumerate(EDGE_FEATURE_MAP):\n",
        "            record[feature_name] = edge_feature_counts_tensor[j].item()\n",
        "\n",
        "        metrics.append(record)\n",
        "\n",
        "    return pd.DataFrame(metrics)"
      ],
      "metadata": {
        "id": "Abfew9DEfPfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67d6ae0d-592d-48fa-b2f9-87c2786d4407",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Main loop\n",
        "# Extracts dataframe for each dataset\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    raise RuntimeError(f\"FATAL ERROR: Path {DATA_DIR} does not exist.\")\n",
        "\n",
        "aggregated_data_cols = ['PDB_ID', 'Affinity', 'Num_Nodes', 'Num_Ligand_Atoms', 'Num_Ligand_Heavy_Atoms', 'Num_Protein_Amino_Acid_Nodes', 'Protein_Amino_Acid_Frequency', 'Num_Edges', 'Density']\n",
        "aggregated_data_df = pd.DataFrame(dict.fromkeys(aggregated_data_cols, []))\n",
        "aggregated_data_df['Dataset_Type'] = []\n",
        "metrics_df_dict = {}\n",
        "node_features_df = pd.DataFrame({'Dataset_Type': []})\n",
        "edge_features_df = pd.DataFrame({'Dataset_Type': []})\n",
        "\n",
        "for i, dataset_type, filename in enumerate(DATASET_FILES_DICT):\n",
        "    data_filepath = os.path.join(DATA_DIR, filename)\n",
        "\n",
        "    if not os.path.exists(data_filepath):\n",
        "        print(f\"WARNING: File not found: {data_filepath}. Skipping...\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Loading {filename} [{i+1}/{len(DATASET_FILES_DICT)}]...\")\n",
        "    try:\n",
        "        # Load data_list\n",
        "        data_list = torch.load(data_filepath)\n",
        "        print(f\"Example data object type: {type(data_list[0])}\")\n",
        "\n",
        "        # Run analysis\n",
        "        metrics_df_dict[dataset_type] = extract_metrics(data_list)\n",
        "        node_features_df = pd.concat([node_features_df, extract_node_features(data_list).assign(Dataset_Type=dataset_type)])\n",
        "        edge_features_df = pd.concat([edge_features_df, extract_edge_features(data_list).assign(Dataset_Type=dataset_type)])\n",
        "        aggregated_data_df = pd.concat([all_data_df, df[aggregated_data_cols].assign(Dataset_Type=dataset_type)])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nFAILED PROCESSING {filename}: {e}\")\n",
        "\n",
        "print(\"--------------| Data extracted |------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Statistics\n",
        "### Dataset Split Counts\n",
        "This section focuses on the basic count stats, to verify the split ratio"
      ],
      "metadata": {
        "id": "NEgLsm8nzByW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(f\"Total Samples: {len(metrics_df_dict.values())}\")\n",
        "train_dataset = metrics_df_dict[\"Train\"]\n",
        "test_dataset = metrics_df_dict[\"Test\"]\n",
        "print(f\"Train/Total Ratio: {len(train_dataset) / len(test_dataset):.2f}\")\n",
        "\n",
        "def dataset_stats(df, dataset_type):\n",
        "  print(f\"{set} Samples ({DATASET_FILES_DICT[dataset_type]}): {len(df)}\")\n",
        "  # print dataframe analysis\n",
        "  df.describe()\n",
        "  unique_complexes = df['PDB_ID'].nunique()\n",
        "  print(f\"\\n[3] Protein-Ligand Complex Diversity: {unique_complexes} unique complexes\")\n",
        "  print(f\"Redundancy Ratio (Total N / Unique N): {len(df) / unique_complexes:.2f}\")\n",
        "\n",
        "\n",
        "dataset_stats(train_dataset, \"Train\")\n",
        "dataset_stats(test_dataset, \"Test\")"
      ],
      "metadata": {
        "id": "xLh9SwcyzAyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target Variable (Binding Affinity) Analysis\n",
        "\n",
        "Compare the distribution of binding affinity values across the datasets.  \n",
        "Goal is for the distributions to be similar across datasets"
      ],
      "metadata": {
        "id": "k6yB_IVzzjz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def target_distribution_analysis(agg_df):\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  for index, row in agg_df.iterrows():\n",
        "    sns.histplot(row['Affinity'], label=f'{row['Dataset']} Set', kde=True, alpha=0.6, stat=\"density\", bins=30) #color='skyblue'\n",
        "  plt.title('Distribution of Target Variable')\n",
        "  plt.xlabel('Binding Affinity (pK)')\n",
        "  plt.ylabel('Density')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.figure(figsize=(8,5))\n",
        "  sns.boxplot(\n",
        "        x='Dataset',\n",
        "        y='Affinity',\n",
        "        data=all_data_df,\n",
        "  )\n",
        "  plt.title('Binding Affinity Distribution Across Datasets')\n",
        "  plt.xlabel('Dataset Split')\n",
        "  plt.ylabel('Binding Affinity ($pK_i$ or $pK_d$)')\n",
        "  plt.grid(axis='y', linestyle='--')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "lH9800o90n4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_distribution_analysis(aggregated_data_df)"
      ],
      "metadata": {
        "id": "DpijgY9TKUV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribution of Target Variable\n",
        "Interpretation: Overlapping histograms demonstrate training and test sets drawn from the same underlyering distribution.  A good split will show very similar density curves.  If the test set's distribution (eg, mean or variance) significantly deviates from the training set, model may perform poorly on generalization, as the test set contains samples not well represented in the training data\n",
        "\n",
        "### Binding Affinity Distribution Across Datasets\n",
        "The median affinity across the datasets is similar, giving confidence that the average binding strength of the test sets is similar to the training set.  A difference would indicate potential bias.  \n",
        "\n",
        "The circles outside of the whiskers indicates the presence of outliers in the training set, which means the model will have to learn from a wide range of values.\n",
        "\n",
        "A follow up will be to validate the outlier data points, to confirm they are not due to errors.\n"
      ],
      "metadata": {
        "id": "AMbvzd3E0sO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph Structure Analysis\n",
        "\n",
        "Examine the structural properties of the graphs\n",
        "\n",
        "Goal: Compare the number of nodes, ligand atoms, protein amnio acids, number of edges, and graph density.  "
      ],
      "metadata": {
        "id": "XFrLPNLi1Ngz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine for plotting\n",
        "def graph_structure_analysis(agg_df):\n",
        "  print(\"\\nGraph Size Metrics\")\n",
        "  fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(8, 5))\n",
        "  fig.suptitle('Graph Structure Analysis', fontsize=16)\n",
        "  sns.violinplot(x='Set', y='Num_Nodes', data=agg_df, ax=axes[0], hue=\"Dataset\", palette='rocket')\n",
        "  plt.title('Distribution of Number of Nodes')\n",
        "  plt.xlabel('Dataset Split')\n",
        "  plt.ylabel('Number of Nodes (Atoms and Amino Acides)')\n",
        "\n",
        "  sns.violinplot(x='Set', y='Num_Ligand_Atoms', data=agg_df, ax=axes[1], hue=\"Dataset\", palette='rocket')\n",
        "  plt.title('Distribution of Number of Ligand Atom Nodes')\n",
        "  plt.xlabel('Dataset Split')\n",
        "  plt.ylabel('Number of Nodes (Ligand Atoms)')\n",
        "\n",
        "  sns.violinplot(x='Set', y='Num_Protein_Amino_Acid_Nodes', data=agg_df, ax=axes[2], hue=\"Dataset\", palette='rocket')\n",
        "  plt.title('Distribution of Number of Protein Amino Acid Nodes')\n",
        "  plt.xlabel('Dataset Split')\n",
        "  plt.ylabel('Number of Nodes (Protein Amino Acid)')\n",
        "\n",
        "  #plot histogram for Num_Interaactions across Datasets\n",
        "  sns.histplot(agg_df[agg_df['Dataset_Type']=='Train']['Num_Edges'], ax=[4], label='Train Set', kde=True, color='skyblue', alpha=0.6, stat=\"Num_Edges\", bins=50)\n",
        "  sns.histplot(agg_df[agg_df['Dataset_Type']=='Test']['Num_Edges'], ax=[5], label='Train Set', kde=True, color='skyblue', alpha=0.6, stat=\"Nun_Edges\", bins=50)\n",
        "\n",
        "  sns.histplot(agg_df[agg_df['Dataset_Type']=='Train']['Density'], ax=[6], label='Train Set', kde=True, color='skyblue', alpha=0.6, stat=\"density\", bins=50)\n",
        "  sns.histplot(agg_df[agg_df['Dataset_Type']=='Test']['Density'], ax=[7], label='Train Set', kde=True, color='skyblue', alpha=0.6, stat=\"density\", bins=50)\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "nmV58erg1nys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribution of Number of Nodes\n",
        "Interpretation: Violin plot shows the spread and density of the atom counts.  If the median and interquartile range for the Test set aligns closely with the train set, it suggests the model will be evaluated on molecules of similar structural complexity to those it was trained on, minimizing generalization risk due to size disparity\n",
        "\n",
        "### Distribution of Edge Counts\n",
        "Interpretation: The comparison of edge counts (bonds) verifies that the complexity of connectivity is balanced.  Significant differences in edge distribution could mean the test set contains graphs that are either much denser or much sparser than the training data, leading to a breakdown of GNN performance that relies on stable message passing"
      ],
      "metadata": {
        "id": "npBRFXSn1w1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Node Feature Analysis\n",
        "\n",
        "Goal: compare the frequency of different atom types or node features"
      ],
      "metadata": {
        "id": "jeDUv_G92Hl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def node_feature_analysis(node_features_df)\n",
        "  train_node_features_df = node_features_df[node_features_df['Dataset_Type']=='Train']\n",
        "  test_node_features_df = node_features_df[node_features_df['Dataset_Type']=='Test']\n",
        "\n",
        "  fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(8, 5))\n",
        "  fig.suptitle('Node Feature Analysis', fontsize=16)\n",
        "  sns.histplot(\n",
        "      data=train_node_features_df,\n",
        "      x='Ligand Heavy Atom Count',\n",
        "      kde=True,\n",
        "      bins=25,\n",
        "      color='#1f77b4',\n",
        "      edgecolor='black',\n",
        "      linewidth=0.8,\n",
        "      ax=axs[0]\n",
        "  )\n",
        "\n",
        "  heavy_atom_mean_count_train = train_node_features_df['Ligand Heavy Atom Count'].mean().item()\n",
        "  plt.title(f'Distribution of Ligand Heavy Atom Counts for Train Set')\n",
        "  plt.xlabel('Number of Heavy Atoms per Ligand', fontsize=12)\n",
        "  plt.ylabel('Frequency (Number of Ligands)', fontsize=12)\n",
        "  plt.axvline(heavy_atom_mean_count_train, ax=axs[0], color='red', linestyle='--', label=f'Mean: {heavy_atom_mean_count:.2f} Atoms')\n",
        "  plt.legend()\n",
        "  plt.grid(axis='y', linestyle=':', alpha=0.6)\n",
        "\n",
        "\n",
        "  sns.histplot(\n",
        "      data=test_node_features_df,\n",
        "      x='Ligand Heavy Atom Count',\n",
        "      kde=True,\n",
        "      bins=25,\n",
        "      color='#1f77b4',\n",
        "      edgecolor='black',\n",
        "      linewidth=0.8,\n",
        "      ax=axs[1]\n",
        "  )\n",
        "  heavy_atom_mean_count_test = test_node_features_df['Ligand Heavy Atom Count'].mean().item()\n",
        "  plt.title(f'Distribution of Ligand Heavy Atom Counts for Test Set')\n",
        "  plt.xlabel('Number of Heavy Atoms per Ligand', fontsize=12)\n",
        "  plt.ylabel('Frequency (Number of Ligands)', fontsize=12)\n",
        "  plt.axvline(heavy_atom_mean_count_train, ax=axs[1], color='red', linestyle='--', label=f'Mean: {heavy_atom_mean_count:.2f} Atoms')\n",
        "  plt.legend()\n",
        "  plt.grid(axis='y', linestyle=':', alpha=0.6)\n",
        "\n",
        "  top_5_features_train = train_node_features_df[LIGAND_FEATURE_MAP].sum().nlargest(5).index.tolist()\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.barplot(x=top_5_features_train, y=train_node_features_df[LIGAND_FEATURE_MAP][top_5_features].sum(), ax=axs[2])\n",
        "  plt.title('Top 5 Ligand Node Features by Sum for Train Set')\n",
        "\n",
        "  top_5_features_test = test_node_features_df[LIGAND_FEATURE_MAP].sum().nlargest(5).index.tolist()\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.barplot(x=top_5_features_test, y=test_node_features_df[LIGAND_FEATURE_MAP][top_5_features].sum(), ax=axs[3])\n",
        "  plt.title('Top 5 Node Ligand Features by Sum for Test Set')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "node_feature_analysis(node_features_df)"
      ],
      "metadata": {
        "id": "UABGqZzW2Xg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation: This analysis ensures feature diversity is consistent across the split.  A large discrepancy in the count of a critical feature in the test set compared with the training set indicates a potential bias.  The model might fail to correctly process those feature types in the test set if it hasn't seen enough examples during training"
      ],
      "metadata": {
        "id": "Mc_4Psur2Yk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Edge Feature Analysis\n",
        "\n",
        "#### Distribution of Number of Edges (Bond Count)\n",
        "\n",
        "Goal: Compare the number of connections in the graphs (related to graph density)"
      ],
      "metadata": {
        "id": "S_lxQSKj24h5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def edge_feature_analysis(edge_features_df)\n",
        "  fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(8, 5))\n",
        "  fig.suptitle('Edge Feature Analysis', fontsize=16)\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  corr_matrix_train = edge_features_df[edge_features_df['Dataset_Type']==\"Train\"].corr()\n",
        "  sns.heatmap(corr_matrix_train, ax=axs[0], annot=True, cmap='coolwarm')\n",
        "  plt.title('Correlation Heatmap of Edge Features for Train Set')\n",
        "\n",
        "  corr_matrix_test = edge_features_df[edge_features_df['Dataset_Type']==\"Test\"].corr()\n",
        "  sns.heatmap(corr_matrix_test, ax=axs[1], annot=True, cmap='coolwarm')\n",
        "  plt.title('Correlation Heatmap of Edge Features for Test Set')\n",
        "\n",
        "  top_5_features = edge_features_df.sum().nlargest(5).index.tolist()\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.barplot(x=top_5_features, y=edge_features_df[top_5_features].sum(), ax=axs[2])\n",
        "  plt.title('Top 5 Edge Features by Sum')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "edge_feature_analysis(edge_features_df)"
      ],
      "metadata": {
        "id": "kknNWzDZ3GA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BVcJs7SQ3Hpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA Scatter plots?"
      ],
      "metadata": {
        "id": "KaIcsJY53anJ"
      }
    }
  ]
}